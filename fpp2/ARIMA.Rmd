---
title: "ARIMA"
output:
  html_document: default
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(fpp2)
library(urca)
```


> ARIMA models provide another approach to time series forecasting. Exponential smoothing and ARIMA models are the two most widely used approaches to time series forecasting, and provide complementary approaches to the problem. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data.

a study notes of [ARIMA](https://otexts.com/fpp2/arima.html)

# Stationary and Differencing

A stationary time serie data is time independent, any trend or seasonal are not stationary, noise is stationary, same as cyclic, because cycle has not fixed length. 

> In general, a stationary time series will have no predictable patterns in the long-term

Differencing: differences between consecutive observations. 

> ACF plot is also useful for identifying non-stationary time series. For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. 

one can do log transform before differencing(multiplicative?)

When differenced data are not stationary, you can try second-order differencing, which is differencing the differces.

#### random walk
>Random walk models are widely used for non-stationary data, particularly financial and economic data. Random walks typically have:

- long periods of apparent trends up or down
- sudden and unpredictable changes in direction.
- forecast from a random walk model is equal to last observation, naive it is then.

#### seasonal differencing
> A seasonal difference is the difference between an observation and the previous observation from the same season

if desire, one can combine seasonal differencing with second-order differencing and/or log transformation.


#### KPSS test, or unit root test
(Kwiatkowski, Phillips, Schmidt, & Shin, 1992)
 
Null hypothesis is data are stationary, samll p-value ( < 0.05) suggest false, then differencing is required. `ruca` package `ur.kpss()`

```{r kpss}
goog %>% ur.kpss() %>% summary
goog %>% diff() %>% ur.kpss() %>% summary()
```

note the different of test statstics after diff

`ndiffs` function suggest number of difference in order to archive stationary.

# Autogressive models
Why it called autogression?  in traditional regression model, outcome predicted against predictor(s)

> we forecast the variable of interest using a linear combination of past values of the variable, The term autoregression indicates that it is a regression of the variable against itself.

`p` order autogressive model, or AR(p) defination: $y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... \phi_p y_{t-p} + e_t$, it is very much like a multi-variant regression.

> We normally restrict autoregressive models to stationary data. When `p is >= 3` the restrictions are much more complicated. R takes care of these restrictions when estimating a model.

# moving average models

>  Rather than using past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model.
# Appendix

`q` order of moving average, or MA(q) equation: $y_t =  c+ e_t + \theta_1 e_{t-1} + \theta_2 e_{t-2} + ... + \theta_q e_{t-q}$

*moving average models should not be confused with the moving average smoothing.*

> A moving average model is used for forecasting future values, while moving average smoothing is used for estimating the trend-cycle of past values.

The fact that AR using observed value and MA using error, makes AR(p) model change scale of series not patterns, MA(q) change patterns not scale. AR and MA models are invertible.

# non-seasonal ARIMA
Combine AR and MA, 'I' stands for integration, which is reverse of differencing, if $y^{'}$ is differenced series(either once or more). then `ARIMA(p,d,q)` is

$y^{'} = c + \phi_1y^{'}_{t-1} + ... \phi_{p}y^{'}_{t-p} + \theta_1e_{t-1} + ... + \theta_q e_{t-q} + e_t$

p is order of autogression, q is order or moving average, d is degree of first differencing involved.

```{r ariam}
fit <- auto.arima(uschange[,"Consumption"], seasonal=FALSE)
summary(fit)
```

so in this ARIMA(1,0,3), the formular is below

$y_t = c + 0.589y_{t-1} - 0.353e_{t-1} + 0.0846e_{t-2}+0.174e_{t-3} +e_t$

where mean times (1-ar1) $c=0.745 (1-0.589)$, e^2 is 0.35

constant `c` effect on long-term foreast:

- If   c = 0 and   d = 0 , the long-term forecasts will go to zero.
- If   c = 0 and   d = 1 , the long-term forecasts will go to a non-zero constant.
- If   c = 0 and   d = 2 , the long-term forecasts will follow a straight line.
- If   c ≠ 0 and   d = 0 , the long-term forecasts will go to the mean of the data.
- If   c ≠ 0 and   d = 1 , the long-term forecasts will follow a straight line.
- If   c ≠ 0 and   d = 2 , the long-term forecasts will follow a quadratic trend.`

```{r arima_1}
fit %>% forecast(h=10) %>% autoplot(include=80)

```

## ACF and partial-ACF
ACF(autocorelatino and cross-correlation function).

sometime ACF or PACF can help show what value of p and q are appropriate for data. ACF show autocorrelations between $y_t$ and $y_{t-k}$ for different values of `k`. 

Now if $y_t$ and  $y_{t − 1}$ are correlated, then $y_{t − 1}$ and $y_{t − 2}$ must also be correlated. However, then $y_t$ and $y_{t − 2}$ might be correlated, simply because they are both connected to $y_{t − 1}$ , rather than because of any new information contained in $y_{t − 2}$ that could be used in forecasting $y_t$ .

*Parital autocorrelations* address this problem by removeing lags 1,2,...,k-1. and measure relationship between $y_t$ and $y_{t-k}$, each parital autocorrelation can be estimated as last coefficient autoregressive model. kth partial autocorrealtion is $\phi_k$ of AR(k). 

>  Each partial autocorrelation can be estimated as the last coefficient in an autoregressive model

In ACF/PACF plot, line over blue line(significant threshold) indicates correlation. in example of the book, PACF has too many big lines, ACF shows 3 lines are over threshold, so to pick simpler model, we should use AR(3). if ACF is smipler, use MA models, you can use both, like the book suggests. [see here for more infor](https://www.youtube.com/watch?v=5Q5p6eVM7zM)

There are rules to identify p,q with ACF or PACF, but `auto.arima` function do it automatically.

## esitmate orders
R using MLE(maximum likelihood estimation) to find parameters c, $\phi_1 ... \phi_p$, and $\theta_1 ... \theta_q$. for ARIAM models MLE is simular to least squares, in practice, R reports log likelihood of data.

> For given values of p, d and q, R will try to maximise the log likelihood when finding parameter estimates.

## information criteria
AIC(Akaike's information criterion) is useful selecting predictors for refressions, is also useful to determine order of ARIAM model. AIC, AICc, and BIC should be miimized value on good model. information critieria not good at selecting differencing(d), only good for  p and q.

# auto.arima()
R involves root test(kpss), minimize AICc and MLE to create aRIMA model. default parameters and orders are defined [here](https://otexts.com/fpp2/arima-r.html)

- approximation = FALSE to speed up search
- stepwise = FALSE to search larger set of models

with Amria() function you can choose model yourself.

## procedure for non-seasonal time series data.

1. Plot the data and identify any unusual observations.
2. If necessary, transform the data (using a Box-Cox transformation) to stabilise the variance.
3. use auto.arima(), or If the data are non-stationary, take first differences of the data until the data are stationary.
4. use auto.arima(), or Examine the ACF/PACF: Is an ARIMA(  p , d , 0 ) or ARIMA(  0 , d , q ) model appropriate? 
5. use auto.arima(), or Try your chosen model(s), and use the AICc to search for a better model.
6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test(in time serie, Ljung-box test) of the residuals. If they do not look like white noise, try a modified model.
7. Once the residuals look like white noise, calculate forecasts.

```{r ariam_1}
# seasadj will add error back to trend, baseicly remove seasons
eeadj <- elecequip %>% stl(s.window = 'periodic') %>% seasadj() 
# data is not stationary, use differenceing
eeadj %>% diff() %>% autoplot()
# PACF suggest AR(3), how? 
```

The PACF suggest AR(3), which I dont get it why, so inital candidata is ARIMA(3,1,0). then we will try ARIMA(4,1,0), ARIMA(2,1,0), and ARIMA(3,1,1), ARIMA(3,1,1) has slightly smaller AICc

```{r arima_2}
(fit <- Arima(eeadj, order=c(3,1,1)))
```
 check if autocorrelations are within threshold

```{r arima_3}
checkresiduals(fit)
```

do a forecast

```{r arima_4}
forecast(fit) %>% autoplot()
```

# Forecasting
too many [equantions](https://otexts.com/fpp2/arima-forecasting.html)

# seasonal ARIMA
seasonal ARIMA writen as ARIAM(p,d,q)(P,D,Q)m, where m is number of observatoins per year.

### ACF/PACF, 
the seasonal part of AR or MA will seen in seasonal lags, for example, ARIMA(0,0,0)(0,0,1)12 would show spike at 12 in ACF, and expontential decay in PACF(12,24,36), ARIMA(0,0,0)(1,0,0)12 would show exponential decay in ACF and a spike at 12 in PACF.

```{r arima_seasonal}
euretail %>% diff(lag=4) %>% ggtsdisplay()
# applied quarterly season, still seems non-stationary
euretail %>% diff(lag=4) %>% diff() %>% ggtsdisplay()
```

now, lag 1 in ACF suggest non-seasonal MA(1),  and lag 4 spike suggest a seasonal MA(1), we can begin with ARIMA(0,1,1)(0,1,1)4.

```{r ariam_seasonal_2}
euretail %>% Arima(order=c(0,1,1), seasonal=c(0,1,1)) %>% residuals() %>% ggtsdisplay()
```

both ACF and PACF show spike at lag2, and almost significant spike at lag(3), it suggest some addition non-seasonal term need to be included. AICc of ARIMA(0,1,3)(0,1,1)4 is 74.36, and ARIMA(0,1,3)(0,1,1)4 is 68.53 other AR term dont produce smaller AICc. consequently we will use ARIMA(0,1,3)(0,1,1)4 model.


```{r ariam_seasonal_3}
fit3 <- Arima(euretail, order=c(0,1,3), seasonal=c(0,1,1))
checkresiduals(fit3)
```

ACF now is within range. forecast next 3 year as following

```{r ariam_seasonal_4}
fit3 %>% forecast(h=12) %>% autoplot()

```

# compares to ETS
all ETS models are non-stationary, while some ARIMA models are stationary.

# Appendix

## Portmanteau test
in time serie, Ljung-box test and Box-Pierce test are 2 variances of portmanteau test, and they are closely connected.  Ljung-Box $H_0$ says data are independent, $H_a$ is data is serial correlated(auto-correaltion)

## Did you forget what is test statistic old man?
significance level (alpha) is chance of $H_0$ get wrongfully rejected, 0.05 is most common value.
in a bell curve plot(assuming two sided hypothesis test), alpha is cut off area of both tail.
the critial calue are cut-off value of tail region, so the test statistic is within the rejection region.

p-value is area of test statistic area on both tail (2 sided), so if p-value > alpha, H0 faled to reject.

[a good illustration here](https://www.geogebra.org/m/YRh9H3t5)
